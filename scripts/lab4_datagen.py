#!/usr/bin/env python3
"""
CLI-based Lab4 FEMA claims data publisher for quickstart-streaming-agents.

Publishes synthetic FEMA claims data to Kafka topic for fraud detection demo.

Usage:
    uv run lab4_datagen
    uv run lab4_datagen --dry-run
    uv run lab4_datagen --cloud-provider aws
"""

import argparse
import csv
import json
import logging
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict

try:
    from confluent_kafka import SerializingProducer
    from confluent_kafka.schema_registry import SchemaRegistryClient
    from confluent_kafka.schema_registry.avro import AvroSerializer
    from confluent_kafka.serialization import StringSerializer
    CONFLUENT_KAFKA_AVAILABLE = True
except ImportError:
    CONFLUENT_KAFKA_AVAILABLE = False

from .common.cloud_detection import auto_detect_cloud_provider, validate_cloud_provider, suggest_cloud_provider
from .common.terraform import extract_kafka_credentials, validate_terraform_state, get_project_root
from .common.logging_utils import setup_logging


class Lab4DataPublisher:
    """Publisher for Lab4 FEMA claims data to Kafka using confluent-kafka library."""

    def __init__(
        self,
        bootstrap_servers: str,
        kafka_api_key: str,
        kafka_api_secret: str,
        schema_registry_url: str,
        schema_registry_api_key: str,
        schema_registry_api_secret: str,
        dry_run: bool = False
    ):
        """Initialize the publisher with Kafka and Schema Registry configuration."""
        self.bootstrap_servers = bootstrap_servers
        self.kafka_api_key = kafka_api_key
        self.kafka_api_secret = kafka_api_secret
        self.schema_registry_url = schema_registry_url
        self.dry_run = dry_run
        self.logger = logging.getLogger(__name__)

        # Create Schema Registry client config
        schema_registry_conf = {
            'url': schema_registry_url,
            'basic.auth.user.info': f'{schema_registry_api_key}:{schema_registry_api_secret}'
        }

        # Initialize Schema Registry client
        self.schema_registry_client = None
        self.avro_serializer = None
        self.string_serializer = StringSerializer('utf_8')

        if not dry_run:
            self.schema_registry_client = SchemaRegistryClient(schema_registry_conf)

            # Use EXACT schema generated by Flink to ensure compatibility
            # Key fields (claim_id, city, claim_amount, claim_timestamp) are NOT NULL
            # to match Lab3 pattern and enable proper primary key inference
            value_schema_str = """{
                "type": "record",
                "name": "claims_value",
                "namespace": "org.apache.flink.avro.generated.record",
                "fields": [
                    {"name": "claim_id", "type": "string"},
                    {"name": "applicant_name", "type": ["null", "string"], "default": null},
                    {"name": "city", "type": "string"},
                    {"name": "is_primary_residence", "type": ["null", "string"], "default": null},
                    {"name": "damage_assessed", "type": ["null", "string"], "default": null},
                    {"name": "claim_amount", "type": "string"},
                    {"name": "has_insurance", "type": ["null", "string"], "default": null},
                    {"name": "insurance_amount", "type": ["null", "string"], "default": null},
                    {"name": "claim_narrative", "type": ["null", "string"], "default": null},
                    {"name": "assessment_date", "type": ["null", "string"], "default": null},
                    {"name": "disaster_date", "type": ["null", "string"], "default": null},
                    {"name": "previous_claims_count", "type": ["null", "string"], "default": null},
                    {"name": "last_claim_date", "type": ["null", "string"], "default": null},
                    {"name": "assessment_source", "type": ["null", "string"], "default": null},
                    {"name": "shared_account", "type": ["null", "string"], "default": null},
                    {"name": "shared_phone", "type": ["null", "string"], "default": null},
                    {"name": "claim_timestamp", "type": {"type": "long", "logicalType": "timestamp-millis"}}
                ]
            }"""

            # Create Avro serializer with minimal type conversions
            def claim_to_avro(claim, ctx):
                """Convert claim dict - only convert timestamp to millis."""
                from datetime import datetime

                claim_copy = claim.copy()

                # Convert timestamp to milliseconds since epoch
                if 'claim_timestamp' in claim_copy and isinstance(claim_copy['claim_timestamp'], str):
                    dt = datetime.fromisoformat(claim_copy['claim_timestamp'])
                    claim_copy['claim_timestamp'] = int(dt.timestamp() * 1000)

                return claim_copy

            self.avro_serializer = AvroSerializer(
                self.schema_registry_client,
                value_schema_str,
                claim_to_avro
            )

        # Create Kafka producer config with serializers
        self.producer_config = {
            'bootstrap.servers': bootstrap_servers,
            'sasl.mechanisms': 'PLAIN',
            'security.protocol': 'SASL_SSL',
            'sasl.username': kafka_api_key,
            'sasl.password': kafka_api_secret,
            'linger.ms': 10,
            'batch.size': 16384,
            'compression.type': 'snappy',
            'key.serializer': self.string_serializer,
            'value.serializer': self.avro_serializer
        }

        # Initialize SerializingProducer (if not dry run)
        self.producer = None
        if not dry_run:
            self.producer = SerializingProducer(self.producer_config)

    def delivery_callback(self, err, msg):
        """Callback for message delivery reports."""
        if err:
            self.logger.error(f"Message delivery failed: {err}")
        else:
            self.logger.debug(f"Message delivered to {msg.topic()} [{msg.partition()}]")

    def publish_claim(self, claim: Dict[str, Any], topic: str) -> bool:
        """
        Publish a single FEMA claim to Kafka using Avro serialization.

        Args:
            claim: Claim data dictionary
            topic: Kafka topic name

        Returns:
            True if successful, False otherwise
        """
        try:
            claim_id = claim['claim_id']

            if self.dry_run:
                self.logger.debug(f"[DRY RUN] Would publish claim {claim_id} to {topic}")
                return True

            # SerializingProducer will automatically call the serializers
            self.producer.produce(
                topic=topic,
                key=claim_id,
                value=claim,
                on_delivery=self.delivery_callback
            )

            # Poll to trigger delivery callbacks
            self.producer.poll(0)

            return True

        except Exception as e:
            self.logger.error(f"Failed to publish claim {claim.get('claim_id', 'unknown')}: {e}")
            self.logger.error(f"Exception type: {type(e).__name__}")
            self.logger.error(f"Claim data: {claim}")
            import traceback
            self.logger.error(f"Full traceback:\n{traceback.format_exc()}")
            return False

    def publish_csv_file(
        self,
        csv_file: Path,
        topic: str,
        simulate_streaming: bool = False,
        speed_multiplier: float = 1.0
    ) -> Dict[str, int]:
        """
        Publish all claims from a CSV file.

        Args:
            csv_file: Path to CSV file with claims
            topic: Kafka topic name
            simulate_streaming: If True, publish claims based on their timestamps
            speed_multiplier: Speed up factor for streaming (1.0 = real-time, 10.0 = 10x faster)

        Returns:
            Dictionary with success/failure counts
        """
        results = {"success": 0, "failed": 0, "total": 0}

        # Read CSV file
        try:
            with open(csv_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                claims = list(reader)
        except Exception as e:
            self.logger.error(f"Failed to read CSV file {csv_file}: {e}")
            return results

        results["total"] = len(claims)
        self.logger.info(f"Found {len(claims)} claims to publish")

        # Sort by timestamp if simulating streaming
        if simulate_streaming:
            claims.sort(key=lambda c: c['claim_timestamp'])
            self.logger.info(f"Simulating streaming at {speed_multiplier}x speed")
            start_time = datetime.fromisoformat(claims[0]['claim_timestamp'])

        # Publish claims
        for idx, claim in enumerate(claims, 1):
            # Simulate streaming timing
            if simulate_streaming and idx > 1:
                current_time = datetime.fromisoformat(claim['claim_timestamp'])
                prev_time = datetime.fromisoformat(claims[idx - 2]['claim_timestamp'])
                time_diff = (current_time - prev_time).total_seconds() / speed_multiplier

                if time_diff > 0 and not self.dry_run:
                    time.sleep(time_diff)

            # Publish claim
            if self.publish_claim(claim, topic):
                results["success"] += 1
            else:
                results["failed"] += 1

            # Periodic flush and progress update
            if not self.dry_run and idx % 50 == 0:
                self.producer.poll(0.1)

            if not self.dry_run and idx % 100 == 0:
                self.producer.flush()
                self.logger.info(
                    f"Progress: {idx}/{results['total']} claims "
                    f"({results['success']} succeeded, {results['failed']} failed)"
                )

        # Final flush
        if not self.dry_run and self.producer:
            self.logger.info("Flushing remaining messages...")
            self.producer.flush()

        return results

    def close(self):
        """Clean up resources."""
        if self.producer:
            self.producer.flush()


def main():
    """Main entry point for the Lab4 data publisher CLI."""
    parser = argparse.ArgumentParser(
        description="Publish Lab4 FEMA claims data to Kafka",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s                                      # Auto-detect cloud provider
  %(prog)s --cloud-provider aws                 # Specify AWS
  %(prog)s --dry-run                            # Test without publishing
  %(prog)s --simulate-streaming --speed 10      # Stream at 10x speed
        """
    )

    parser.add_argument(
        "--cloud-provider",
        choices=["aws", "azure"],
        help="Target cloud provider (auto-detected if not specified)"
    )
    parser.add_argument(
        "--data-file",
        type=Path,
        help="Path to CSV data file (default: auto-detect in data-gen directory)"
    )
    parser.add_argument(
        "--topic",
        default="claims",
        help="Kafka topic name (default: claims)"
    )
    parser.add_argument(
        "--simulate-streaming",
        action="store_true",
        help="Publish claims based on timestamps (simulates real-time streaming)"
    )
    parser.add_argument(
        "--speed",
        type=float,
        default=1.0,
        help="Speed multiplier for streaming simulation (default: 1.0 = real-time)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Test without actually publishing"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose logging"
    )

    args = parser.parse_args()

    # Set up logging
    logger = setup_logging(args.verbose)

    # Check confluent-kafka library is available
    if not CONFLUENT_KAFKA_AVAILABLE:
        logger.error("confluent-kafka library not available. Please install it with: uv pip install confluent-kafka")
        return 1

    # Get project root
    try:
        project_root = get_project_root()
    except Exception as e:
        logger.error(f"Could not find project root: {e}")
        return 1

    # Find data file
    if args.data_file:
        data_file = args.data_file
    else:
        # Auto-detect in data-gen directory
        data_file = project_root / "terraform" / "lab4-pubsec-fraud-agents" / "data-gen" / "fema_claims_synthetic.csv"

    if not data_file.exists():
        logger.error(f"Data file does not exist: {data_file}")
        logger.error("Please run the data generation first:")
        logger.error("  cd terraform/lab4-pubsec-fraud-agents/data-gen")
        logger.error("  uv run python generate_fema_claims_data.py")
        return 1

    logger.info(f"Publishing Lab4 FEMA claims data from {data_file}")

    # Determine cloud provider
    cloud_provider = args.cloud_provider
    if not cloud_provider:
        cloud_provider = auto_detect_cloud_provider()
        if not cloud_provider:
            suggestion = suggest_cloud_provider(project_root)
            if suggestion:
                logger.info(f"Auto-detected cloud provider: {suggestion}")
                cloud_provider = suggestion
            else:
                logger.error("Could not auto-detect cloud provider. Please check your terraform deployment.")
                return 1

    logger.info(f"Target cloud provider: {cloud_provider.upper()}")

    # Validate cloud provider
    if not validate_cloud_provider(cloud_provider):
        logger.error(f"Invalid cloud provider: {cloud_provider}")
        return 1

    # Validate terraform state
    try:
        if not validate_terraform_state(cloud_provider, project_root):
            logger.error("Terraform validation failed")
            return 1
    except Exception as e:
        logger.error(f"Terraform validation failed: {e}")
        return 1

    # Extract Kafka credentials
    try:
        credentials = extract_kafka_credentials(cloud_provider, project_root)
    except Exception as e:
        logger.error(f"Failed to extract Kafka credentials: {e}")
        return 1

    # Initialize publisher
    try:
        publisher = Lab4DataPublisher(
            bootstrap_servers=credentials["bootstrap_servers"],
            kafka_api_key=credentials["kafka_api_key"],
            kafka_api_secret=credentials["kafka_api_secret"],
            schema_registry_url=credentials["schema_registry_url"],
            schema_registry_api_key=credentials["schema_registry_api_key"],
            schema_registry_api_secret=credentials["schema_registry_api_secret"],
            dry_run=args.dry_run
        )
    except Exception as e:
        logger.error(f"Failed to initialize publisher: {e}")
        return 1

    # Publish data
    try:
        logger.info(f"Publishing FEMA claims to topic '{args.topic}'")
        if args.dry_run:
            logger.info("[DRY RUN MODE - No actual publishing will occur]")

        results = publisher.publish_csv_file(
            data_file,
            args.topic,
            simulate_streaming=args.simulate_streaming,
            speed_multiplier=args.speed
        )

        print(f"\n{'=' * 60}")
        print("LAB4 DATA PUBLISHING SUMMARY")
        print(f"{'=' * 60}")
        print(f"Total claims:     {results['total']}")
        print(f"Published:        {results['success']}")
        print(f"Failed:           {results['failed']}")
        print(f"{'=' * 60}")

        if args.dry_run:
            print("\n[DRY RUN COMPLETE - No messages were actually published]")
        else:
            print(f"\nâœ… Published {results['success']} FEMA claims to '{args.topic}' topic")
            print(f"Ready for Flink anomaly detection!")

        return 0 if results['failed'] == 0 else 1
    finally:
        publisher.close()


if __name__ == "__main__":
    sys.exit(main())
