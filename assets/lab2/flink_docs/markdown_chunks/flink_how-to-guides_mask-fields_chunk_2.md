---
document_id: flink_how-to-guides_mask-fields_chunk_2
source_file: flink_how-to-guides_mask-fields.md
source_url: https://docs.confluent.io/cloud/current/flink/how-to-guides/mask-fields.html
title: Mask Fields in a Table with Confluent Cloud for Apache Flink
chunk_index: 2
total_chunks: 2
---

Throughway 58261 Dickinsonburgh <ivory.lockman@gmail.com> ...

## Step 3: Apply the Mask Fields action¶

In the previous step, you created a Flink table that had rows with customer names, which might be confidential data. In this step, you apply the Mask Fields action to create an output table that has the contents of the `name` field masked.

  1. Navigate to the [Environments](https://confluent.cloud/environments) page, and in the navigation menu, click **Data portal**.

  2. In the **Data portal** page, click the dropdown menu and select the environment for your workspace.

  3. In the **Recently created** section, find your **customers_source** topic and click it to open the details pane.

  4. Click **Actions** , and in the Actions list, click **Mask fields** to open the **Mask fields** dialog.

  5. In the **Field to mask** dropdown, select **name**.

  6. In the **Regex for name** dropdown, select **Word characters**.

  7. In the **Runtime configuration** section, either select an existing service account or create a new service account for the current action.

Note

The service you select must have the EnvironmentAdmin role to create topics, schemas, and run Flink statements.

  8. Optionally, click the **Show SQL** toggle to view the statements that the action will run.

The code resembles:

         CREATE TABLE `<your-environment>`.`<your-kafka-cluster>`.`customers_source_mask`
           LIKE `<your-environment>`.`<your-kafka-cluster>`.`customers_source`

         INSERT INTO `<your-environment>`.`<your-kafka-cluster>`.`customers_source_mask` SELECT
           `customer_id`,
           REGEXP_REPLACE(`name`, '(\w)', '*') as `name`,
           address,
           postcode,
           city,
           email
         FROM `<your-environment>`.`<your-kafka-cluster>`.`customers_source`;

  9. Click **Confirm**.

The action runs the CREATE TABLE and INSERT INTO statements. These statements register the `customers_source_mask` table and populate it with rows from the `customers_source` table. The strings in the `name` column are masked by the [REGEXP_REPLACE](../reference/functions/string-functions.html#flink-sql-regexp-replace-function) function.

## Step 4: Inspect the output table¶

The statements that were generated by the Mask Fields action created an output table named `customers_source_mask`. In this step, you query the output table to see the masked field values.

* Return to your workspace and run the following command to inspect the `customers_source_mask` output table.

        SELECT * FROM customers_source_mask;

Your output should resemble:

        customer_id name                 address                postcode city              email
        3104        **** *** ******      342 Odis Hollow        27615    West Florentino   bryce.hodkiewicz@hotmail.c…
        3058        **** ******* ******  33569 Turner Glens     14107    Schummchester     sarah.roob@yahoo.com
        3138        **** ****** ******** 944 Elden Walks        39293    New Ernestbury    velvet.volkman@gmail.com
        ...

## Step 5: Stop the persistent query¶

The INSERT INTO statement that was created by the Mask Fields action runs continuously until you stop it manually. Free resources in your compute pool by deleting the long-running statement.

  1. Navigate to the **Flink** page in your environment and click **Flink statements**.
  2. In the statements list, find the statement that has a status of **Running**.
  3. In the **Actions** column, click **…** and select **Delete statement**.
  4. In the **Confirm statement deletion** dialog, copy and paste the statement name and click **Confirm**.
